import json
import os
import logging
from typing import Dict, List, Optional, Tuple
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel, Field
import random
from difflib import SequenceMatcher
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss

from dotenv import load_dotenv
# ============================================
# CONFIGURATION ET LOGGING
# ============================================

load_dotenv()
print("GROQ_API_KEY =", os.getenv("GROQ_API_KEY"))

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('anontchigan.log', encoding='utf-8')
    ]
)
logger = logging.getLogger("ANONTCHIGAN")

class Config:
    """Configuration optimis√©e pour √©viter les coupures"""
    SIMILARITY_THRESHOLD = 0.75
    MAX_HISTORY_LENGTH = 8
    MAX_CONTEXT_LENGTH = 1000
    MAX_ANSWER_LENGTH = 600  # Augment√© significativement
    FAISS_RESULTS_COUNT = 3
    MIN_ANSWER_LENGTH = 30

# ============================================
# MOD√àLES DE DONN√âES
# ============================================

class ChatQuery(BaseModel):
    question: str = Field(..., min_length=1, max_length=500)
    user_id: str = Field(..., min_length=1)

class ChatResponse(BaseModel):
    answer: str
    status: str
    method: str
    score: Optional[float] = None
    matched_question: Optional[str] = None
    context_used: Optional[int] = None

# ============================================
# SERVICE GROQ CORRIG√â
# ============================================

class GroqService:
    
    def __init__(self):
        self.client = None
        self.available = False
        self._initialize_groq()
    
    







    def _initialize_groq(self):
        try:
            from groq import Groq

            api_key = os.getenv("GROQ_API_KEY")
            if not api_key:
                logger.warning("‚ö†Ô∏è Cl√© API Groq manquante ‚Äî v√©rifie la variable d'environnement GROQ_API_KEY")
                return

            # ‚úÖ Initialisation sans argument 'proxies'
            self.client = Groq(api_key=api_key)

            # ‚úÖ Petit test de requ√™te pour confirmer la disponibilit√©
            test_response = self.client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[{"role": "user", "content": "test"}],
                max_tokens=5
            )

            if test_response:
                self.available = True
                logger.info("‚úÖ Service Groq initialis√© et op√©rationnel")
            else:
                self.available = False
                logger.warning("‚ö†Ô∏è Test Groq √©chou√©, client non disponible")

        except Exception as e:
            self.available = False
            logger.warning(f"‚ùå Service Groq non disponible : {str(e)}")








    def generate_response(self, question: str, context: str, history: List[Dict]) -> str:
        """G√©n√®re une r√©ponse compl√®te sans coupure"""
        if not self.available:
            raise RuntimeError("Service Groq non disponible")
        
        try:
            # Pr√©parer le contexte optimis√©
            context_short = self._prepare_context(context)
            
            # Pr√©parer les messages
            messages = self._prepare_messages(question, context_short, history)
            
            logger.info("ü§ñ G√©n√©ration avec Groq...")
            
            # AUGMENTER SIGNIFICATIVEMENT les tokens pour √©viter les coupures
            response = self.client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=messages,
                max_tokens=600,  # Augment√© pour √©viter coupures
                temperature=0.7,
                top_p=0.9,
            )
            
            answer = response.choices[0].message.content.strip()
            answer = self._clean_response(answer)
            
            # Validation renforc√©e
            if not self._is_valid_answer(answer):
                raise ValueError("R√©ponse trop courte")
                
            # V√©rification et correction des coupures
            answer = self._ensure_complete_response(answer)
            
            logger.info(f"‚úì R√©ponse g√©n√©r√©e ({len(answer)} caract√®res)")
            return answer
            
        except Exception as e:
            logger.error(f"Erreur Groq: {str(e)}")
            raise
    
    def _prepare_context(self, context: str) -> str:
        """Pr√©pare un contexte concis"""
        lines = context.split('\n')[:5]  # R√©duit pour focus
        context_short = '\n'.join(lines)
        if len(context_short) > Config.MAX_CONTEXT_LENGTH:
            context_short = context_short[:Config.MAX_CONTEXT_LENGTH-3] + "..."
        return context_short
    
    def _prepare_messages(self, question: str, context: str, history: List[Dict]) -> List[Dict]:
        """Pr√©pare les messages avec instructions claires contre les coupures"""
        
        system_prompt = f"""Tu es ANONTCHIGAN, assistante IA professionnelle sp√©cialis√©e dans la sensibilisation au cancer du sein au B√©nin.

CONTEXTE √Ä UTILISER :
{context}

R√àGLES CRITIQUES :
1. FOURNIR DES R√âPONSES COMPL√àTES - ne JAMAIS couper une phrase en milieu de mot
2. Si tu donnes un conseil de pr√©vention, aller deux fois √† la ligne AVANT de le donner
3. Tes r√©ponses doivent se terminer naturellement par un point final

STYLE :
- Professionnel, clair, empathique
- R√©ponses directes sans formules introductives
- CONCIS mais COMPLET
- Humour humain l√©ger 1 fois sur 3 r√©ponses
- Emojis √† utiliser fr√©quemment : üíó üå∏ üòä üáßüáØ

STRUCTURE DES R√âPONSES :
1. R√©ponse principale bas√©e sur le contexte
2- N'invente PAS d'informations
3- Si le contexte ne couvre pas la question, recommande de consulter un professionnel de sant√© au B√©nin
4- ENSGMM se d√©finit comme l'Ecole Nationale Sup√©rieure de G√©nie Math√©matique et Mod√©lisation 
5- Utilise des connecteurs logiques pour fluidifier et produire des r√©ponses naturelles et coh√©rentes

DIRECTIVES ANTI-CONPUTURE :
- V√©rifie que ta r√©ponse est compl√®te avant de terminer
- Ne coupe PAS en milieu de phrase ou de mot
- Utilise "Atassa!" ou "Atassaaaaa!" en debut de phrase en cas dh'humour et d'√©tonnemnt extr√™me
- Termine par un point final appropri√©
- Si tu mentionnes des noms (cr√©ateurs, etc.), assure-toi qu'ils sont COMPLETS

Conseils de pr√©vention : seulement si pertinents et si demand√©s."""

        messages = [{"role": "system", "content": system_prompt}]
        
        # Historique r√©cent
        for msg in history[-4:]:
            messages.append(msg)
        
        # Question actuelle avec instruction explicite
        messages.append({
            "role": "user", 
            "content": f"QUESTION: {question}\n\nIMPORTANT : R√©ponds de fa√ßon COMPL√àTE sans couper ta r√©ponse. Termine par un point final. Si conseil de pr√©vention, va √† la ligne avant."
        })
        
        return messages
    
    def _clean_response(self, answer: str) -> str:
        """Nettoie la r√©ponse en gardant la personnalit√©"""
        
        # Supprimer les introductions verbeuses
        unwanted_intros = [
            'bonjour', 'salut', 'coucou', 'hello', 'akw√®', 'yo', 'bonsoir', 'hi',
            'excellente question', 'je suis ravi', 'permettez-moi', 'tout d abord',
            'premi√®rement', 'pour commencer', 'en tant qu', 'je suis anontchigan'
        ]
        
        answer_lower = answer.lower()
        for phrase in unwanted_intros:
            if answer_lower.startswith(phrase):
                sentences = answer.split('.')
                if len(sentences) > 1:
                    answer = '.'.join(sentences[1:]).strip()
                    if answer:
                        answer = answer[0].upper() + answer[1:]
                break
        
        return answer.strip()
    
    def _is_valid_answer(self, answer: str) -> bool:
        """Valide que la r√©ponse est acceptable"""
        return (len(answer) >= Config.MIN_ANSWER_LENGTH and 
                not answer.lower().startswith(('je ne sais pas', 'd√©sol√©', 'sorry')))
    
    def _ensure_complete_response(self, answer: str) -> str:
        """Garantit que la r√©ponse est compl√®te et non coup√©e"""
        if not answer:
            return answer
            
        # D√©tecter les signes de coupure
        cut_indicators = [
            answer.endswith('...'),
            answer.endswith(','),
            answer.endswith(';'),
            answer.endswith(' '),
            any(word in answer.lower() for word in ['http', 'www.', '.com']),  # URLs coup√©es
            '...' in answer[-10:]  # Points de suspension vers la fin
        ]
        
        if any(cut_indicators):
            logger.warning("‚ö†Ô∏è  D√©tection possible de r√©ponse coup√©e")
            
            # Trouver la derni√®re phrase compl√®te
            last_period = answer.rfind('.')
            last_exclamation = answer.rfind('!')
            last_question = answer.rfind('?')
            
            sentence_end = max(last_period, last_exclamation, last_question)
            
            if sentence_end > 0 and sentence_end >= len(answer) - 5:
                # Garder jusqu'√† la derni√®re phrase compl√®te
                answer = answer[:sentence_end + 1]
            else:
                # Si pas de ponctuation claire, nettoyer la fin
                answer = answer.rstrip(' ,;...')
                if not answer.endswith(('.', '!', '?')):
                    answer += '.'
        
        # Formater les conseils de pr√©vention avec saut de ligne
        prevention_phrases = [
            'conseil de pr√©vention',
            'pour pr√©venir',
            'je recommande',
            'il est important de',
            'n oubliez pas de'
        ]
        
        # V√©rifier si un conseil de pr√©vention est pr√©sent
        has_prevention_advice = any(phrase in answer.lower() for phrase in prevention_phrases)
        
        if has_prevention_advice:
            # Essayer d'ins√©rer un saut de ligne avant le conseil
            lines = answer.split('. ')
            if len(lines) > 1:
                # Trouver la ligne qui contient le conseil
                for i, line in enumerate(lines[1:], 1):
                    if any(phrase in line.lower() for phrase in prevention_phrases):
                        # Ins√©rer un saut de ligne avant cette ligne
                        lines[i] = '\n' + lines[i]
                        answer = '. '.join(lines)
                        break
        
        return answer

# ============================================
# SERVICES RAG ET CONVERSATION
# ============================================

class RAGService:
    """Service RAG avec recherche am√©lior√©e"""
    
    def __init__(self, data_file: str = 'cancer_sein.json'):
        self.questions_data = []
        self.embedding_model = None
        self.index = None
        self.embeddings = None
        self._load_data(data_file)
        self._initialize_embeddings()
    
    def _load_data(self, data_file: str):
        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            for item in data:
                self.questions_data.append({
                    'question_originale': item['question'],
                    'question_normalisee': item['question'].lower().strip(),
                    'answer': item['answer']
                })
            
            logger.info(f"‚úì {len(self.questions_data)} questions charg√©es")
            
        except Exception as e:
            logger.error(f"Erreur chargement donn√©es: {str(e)}")
            raise
    
    def _initialize_embeddings(self):
        try:
            self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            
            all_texts = [
                f"Q: {item['question_originale']} R: {item['answer']}"
                for item in self.questions_data
            ]
            
            self.embeddings = self.embedding_model.encode(all_texts, show_progress_bar=False)
            self.embeddings = np.array(self.embeddings).astype('float32')
            
            dimension = self.embeddings.shape[1]
            self.index = faiss.IndexFlatL2(dimension)
            self.index.add(self.embeddings)
            
            logger.info(f"‚úì Index FAISS cr√©√© ({len(self.embeddings)} vecteurs)")
            
        except Exception as e:
            logger.error(f"Erreur initialisation embeddings: {str(e)}")
            raise
    
    def search(self, query: str, k: int = Config.FAISS_RESULTS_COUNT) -> List[Dict]:
        """Recherche optimis√©e dans FAISS"""
        try:
            query_embedding = self.embedding_model.encode([query])
            query_embedding = np.array(query_embedding).astype('float32')
            
            distances, indices = self.index.search(query_embedding, k)
            
            results = []
            for i, idx in enumerate(indices[0]):
                if idx < len(self.questions_data):
                    similarity = 1 / (1 + distances[0][i])
                    results.append({
                        'question': self.questions_data[idx]['question_originale'],
                        'answer': self.questions_data[idx]['answer'],
                        'similarity': similarity,
                        'distance': distances[0][i]
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"Erreur recherche FAISS: {str(e)}")
            return []

class ConversationManager:
    """Gestionnaire de conversations"""
    
    def __init__(self):
        self.conversations: Dict[str, List[Dict]] = {}
    
    def get_history(self, user_id: str) -> List[Dict]:
        return self.conversations.get(user_id, [])
    
    def add_message(self, user_id: str, role: str, content: str):
        if user_id not in self.conversations:
            self.conversations[user_id] = []
        
        self.conversations[user_id].append({"role": role, "content": content})
        
        if len(self.conversations[user_id]) > Config.MAX_HISTORY_LENGTH * 2:
            self.conversations[user_id] = self.conversations[user_id][-Config.MAX_HISTORY_LENGTH * 2:]

# ============================================
# INITIALISATION ET ENDPOINTS
# ============================================

groq_service = GroqService()
rag_service = RAGService()
conversation_manager = ConversationManager()

@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("üöÄ D√©marrage d'ANONTCHIGAN anti-coupure...")
    yield
    logger.info("üõë Arr√™t d'ANONTCHIGAN...")

app = FastAPI(
    title="ANONTCHIGAN API",
    description="Assistant IA pour la sensibilisation au cancer du sein",
    version="2.2.0",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def serve_home():
    return FileResponse("index.html")

@app.get("/health")
async def health_check():
    return {
        "status": "healthy", 
        "version": "2.2.0",
        "groq_available": groq_service.available,
        "optimizations": ["anti_coupure", "conseils_formates", "reponses_completes"]
    }

@app.post("/chat")
async def chat(query: ChatQuery):
    logger.info(f"üì• Question: {query.question}")
    
    try:
        history = conversation_manager.get_history(query.user_id)
        
        # Gestion des salutations
        salutations = ["cc","bonjour", "salut", "coucou", "hello", "akwe", "yo", "bonsoir", "hi"]
        question_lower = query.question.lower().strip()
        
        if any(salut == question_lower for salut in salutations):
            responses = [
                "Je suis ANONTCHIGAN, assistante pour la sensibilisation au cancer du sein. Comment puis-je vous aider ? üíó",
                "Bonjour ! Je suis ANONTCHIGAN. Que souhaitez-vous savoir sur le cancer du sein ? üå∏",
                "ANONTCHIGAN √† votre service. Posez-moi vos questions sur la pr√©vention du cancer du sein. üòä"
            ]
            answer = random.choice(responses)
            
            conversation_manager.add_message(query.user_id, "user", query.question)
            conversation_manager.add_message(query.user_id, "assistant", answer)
            
            return ChatResponse(
                answer=answer,
                status="success",
                method="salutation"
            )
        
        # Recherche FAISS
        logger.info("üîç Recherche FAISS...")
        faiss_results = rag_service.search(query.question)
        
        if not faiss_results:
            answer = "Les informations disponibles ne couvrent pas ce point sp√©cifique. Je vous recommande de consulter un professionnel de sant√© au B√©nin pour des conseils adapt√©s. üíó"
            conversation_manager.add_message(query.user_id, "user", query.question)
            conversation_manager.add_message(query.user_id, "assistant", answer)
            
            return ChatResponse(
                answer=answer,
                status="info",
                method="no_result"
            )
        
        best_result = faiss_results[0]
        similarity = best_result['similarity']
        
        logger.info(f"üìä Meilleure similarit√©: {similarity:.3f}")
        
        # D√©cision : R√©ponse directe vs G√©n√©ration
        if similarity >= Config.SIMILARITY_THRESHOLD:
            logger.info(f"‚úÖ Haute similarit√© ‚Üí R√©ponse directe")
            answer = best_result['answer']
            
            # S'assurer que les r√©ponses directes ne sont pas coup√©es non plus
            if len(answer) > Config.MAX_ANSWER_LENGTH:
                answer = answer[:Config.MAX_ANSWER_LENGTH-3] + "..."
            
            conversation_manager.add_message(query.user_id, "user", query.question)
            conversation_manager.add_message(query.user_id, "assistant", answer)
            
            return ChatResponse(
                answer=answer,
                status="success",
                method="json_direct",
                score=float(similarity),
                matched_question=best_result['question']
            )
        else:
            logger.info(f"ü§ñ Similarit√© mod√©r√©e ‚Üí G√©n√©ration Groq")
            
            # Pr√©parer le contexte
            context_parts = []
            for i, result in enumerate(faiss_results[:3], 1):
                answer_truncated = result['answer']
                if len(answer_truncated) > 200:  # R√©duit pour laisser plus de place √† la g√©n√©ration
                    answer_truncated = answer_truncated[:197] + "..."
                context_parts.append(f"{i}. Q: {result['question']}\n   R: {answer_truncated}")
            
            context = "\n\n".join(context_parts)
            
            # G√©n√©ration avec Groq
            try:
                if groq_service.available:
                    generated_answer = groq_service.generate_response(query.question, context, history)
                else:
                    generated_answer = "Je vous recommande de consulter un professionnel de sant√© pour cette question sp√©cifique. La pr√©vention pr√©coce est essentielle. üíó"
            except Exception as e:
                logger.warning(f"G√©n√©ration √©chou√©e: {str(e)}")
                generated_answer = "Pour des informations pr√©cises sur ce sujet, veuillez consulter un m√©decin ou un centre de sant√© sp√©cialis√© au B√©nin. üå∏"
            
            conversation_manager.add_message(query.user_id, "user", query.question)
            conversation_manager.add_message(query.user_id, "assistant", generated_answer)
            
            return ChatResponse(
                answer=generated_answer,
                status="success",
                method="groq_generated",
                score=float(similarity),
                context_used=len(faiss_results[:3])
            )
            
    except Exception as e:
        logger.error(f"‚ùå Erreur: {str(e)}")
        error_message = "D√©sol√©, une erreur s'est produite. Veuillez r√©essayer."
        
        conversation_manager.add_message(query.user_id, "user", query.question)
        conversation_manager.add_message(query.user_id, "assistant", error_message)
        
        return ChatResponse(
            answer=error_message,
            status="error",
            method="error"
        )

if __name__ == "__main__":
    import uvicorn
    
    logger.info("\n" + "="*50)
    logger.info("‚úì ANONTCHIGAN ANTI-COUPURE - Pr√™t!")
    logger.info("  - Max tokens: 550 (augment√©)")
    logger.info("  - D√©tection coupures: Activ√©e")
    logger.info("  - Conseils format√©s: Avec sauts de ligne")
    logger.info(f"  - G√©n√©ration: {'Groq ‚ö°' if groq_service.available else 'Fallback'}")
    logger.info("="*50 + "\n")
    
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False)